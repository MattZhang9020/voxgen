{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR_PTH = \".\\\\dataset\\\\partnet\\\\chair_voxel_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, dataset_dir_pth, size=None, batch_size=4, voxel_map_shape=(128, 128, 128)):\n",
    "        self.dataset_dir_pth = dataset_dir_pth\n",
    "        self.data_names = np.array(sorted_alphanumeric(os.listdir(self.dataset_dir_pth)), dtype=str)[:size]\n",
    "        self.num_samples = len(self.data_names)\n",
    "        self.batch_size = batch_size\n",
    "        self.curr_index = 0\n",
    "        self.indexes = np.arange(int(self.num_samples / self.batch_size) * self.batch_size)\n",
    "        self.voxel_map_shape = voxel_map_shape\n",
    "        self.voxel_data_sparse = self._load_voxel_data()\n",
    "        \n",
    "        np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def _load_voxel_data(self):\n",
    "        voxel_data_sparse = []\n",
    "        for data_name in tqdm(self.data_names, desc=\"Loading Voxel Data\"):\n",
    "            data_pth = os.path.join(self.dataset_dir_pth, data_name)\n",
    "            voxel_data_sparse.append(np.load(data_pth))\n",
    "        return voxel_data_sparse\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.num_samples / self.batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def _data_to_dense(self, objs_voxel_data):\n",
    "        dense_objs_voxel_data = []\n",
    "        \n",
    "        for obj_voxel_coordinates in objs_voxel_data:\n",
    "            voxel_map = np.zeros(self.voxel_map_shape, dtype=np.float32)\n",
    "            voxel_map[tuple(obj_voxel_coordinates.T)] = 1.0\n",
    "            dense_objs_voxel_data.append(voxel_map)\n",
    "            \n",
    "        return dense_objs_voxel_data\n",
    "\n",
    "    # def _rotate_obj_voxel_data(self, coordinates, rotation_angle_range):\n",
    "    #     rotation_angle = random.randint(-rotation_angle_range, rotation_angle_range)\n",
    "        \n",
    "    #     theta = np.pi * (rotation_angle / 180)\n",
    "        \n",
    "    #     rotation_matrix = np.array([[np.cos(theta), 0, np.sin(theta)],\n",
    "    #                                 [0, 1, 0],\n",
    "    #                                 [-np.sin(theta), 0, np.cos(theta)]])\n",
    "        \n",
    "    #     center = np.array(self.voxel_map_shape) / 2\n",
    "    #     translated_coords = coordinates - center\n",
    "        \n",
    "    #     rotated_coords = np.dot(translated_coords, rotation_matrix.T)\n",
    "    #     rotated_coords += center\n",
    "        \n",
    "    #     rotated_coords = np.round(rotated_coords).astype(int)\n",
    "    #     valid_indices = np.all((rotated_coords >= 0) & (rotated_coords < np.array(self.voxel_map_shape)), axis=1)\n",
    "        \n",
    "    #     rotated_coords = rotated_coords[valid_indices]\n",
    "        \n",
    "    #     return rotated_coords\n",
    "        \n",
    "    # def _random_transform(self, objs_voxel_data, rotation_angle_range=45, probability=1):\n",
    "    #     transformed_objs_voxel_data = []\n",
    "\n",
    "    #     for obj_voxel_coordinates in objs_voxel_data:\n",
    "            \n",
    "    #         if random.random() < probability:\n",
    "    #             obj_voxel_coordinates = self._rotate_obj_voxel_data(obj_voxel_coordinates, rotation_angle_range)\n",
    "\n",
    "    #         transformed_objs_voxel_data.append(obj_voxel_coordinates)\n",
    "\n",
    "    #     return transformed_objs_voxel_data\n",
    "\n",
    "    def _load_batched_sparse_data(self, indexes):\n",
    "        voxel_data_sparse = []\n",
    "        \n",
    "        if self.voxel_data_sparse is not None:\n",
    "            for i in indexes:\n",
    "                voxel_data_sparse.append(self.voxel_data_sparse[i])\n",
    "                \n",
    "            return voxel_data_sparse\n",
    "        else:\n",
    "            indexed_data_names = self.data_names[indexes]\n",
    "\n",
    "            for data_name in indexed_data_names:\n",
    "                data_pth = os.path.join(self.dataset_dir_pth, data_name)\n",
    "\n",
    "                voxel_data_sparse.append(np.load(data_pth))  # attention ! bottleneck here\n",
    "\n",
    "            return voxel_data_sparse\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.curr_index >= len(self.indexes):\n",
    "            np.random.shuffle(self.indexes)\n",
    "            self.curr_index = 0\n",
    "            raise StopIteration\n",
    "            \n",
    "        batched_indexes = self.indexes[self.curr_index: self.curr_index + self.batch_size]\n",
    "        \n",
    "        batched_sparse_data = self._load_batched_sparse_data(batched_indexes)\n",
    "                \n",
    "        batched_dense_data = self._data_to_dense(batched_sparse_data)\n",
    "\n",
    "        self.curr_index += self.batch_size\n",
    "        \n",
    "        return batched_indexes, batched_dense_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(dataset_dir_pth=DATASET_DIR_PTH, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hparam = {\n",
    "    'model_latent_code_dim': 256,   \n",
    "    'model_fc_channels': 512,\n",
    "    'model_fc_activation': 'relu',\n",
    "    'model_norm_3d': 'batchnorm',\n",
    "    'model_conv_size': 4,\n",
    "    'model_num_latent_codes': len(data_generator) * BATCH_SIZE,\n",
    "    'model_learning_rate_network': 1e-4,\n",
    "    'model_learning_rate_codes': 1e-3,\n",
    "    'model_checkpoint_dir': './ckpt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeometryNetwork:\n",
    "    def __init__(self, hparam):\n",
    "        self.latent_code_dim = hparam['model_latent_code_dim']\n",
    "        self.fc_channels = hparam['model_fc_channels']\n",
    "        self.fc_activation = hparam['model_fc_activation']\n",
    "        self.norm_3d = hparam['model_norm_3d']\n",
    "        self.conv_size = hparam['model_conv_size']\n",
    "        self.num_latent_codes = hparam['model_num_latent_codes']\n",
    "        self.learning_rate_network = hparam['model_learning_rate_network']\n",
    "        self.learning_rate_codes = hparam['model_learning_rate_codes']\n",
    "        self.checkpoint_dir = hparam['model_checkpoint_dir']\n",
    "        \n",
    "        self.init_model()\n",
    "        self.init_optimizer()\n",
    "        self.init_losser()\n",
    "        self.init_checkpoint()\n",
    "\n",
    "    def norm_layer(self, tensor, normalization):\n",
    "        if normalization and normalization.lower() == 'batchnorm':\n",
    "            tensor = tf.keras.layers.BatchNormalization()(tensor)\n",
    "        return tensor\n",
    "\n",
    "    def conv_t_block_3d(self,\n",
    "                        tensor,\n",
    "                        num_filters,\n",
    "                        size,\n",
    "                        strides,\n",
    "                        normalization=None,\n",
    "                        dropout=False,\n",
    "                        alpha_lrelu=0.2,\n",
    "                        relu=True,\n",
    "                        rate=0.7):\n",
    "        conv_3D_transpose = tf.keras.layers.Conv3DTranspose(num_filters,\n",
    "                                                            size,\n",
    "                                                            strides=strides,\n",
    "                                                            padding='same',\n",
    "                                                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "                                                            use_bias=False)\n",
    "\n",
    "        tensor = conv_3D_transpose(tensor)\n",
    "\n",
    "        tensor = self.norm_layer(tensor, normalization)\n",
    "\n",
    "        if relu:\n",
    "            tensor = tf.keras.layers.LeakyReLU(alpha=alpha_lrelu)(tensor)\n",
    "\n",
    "        if dropout:\n",
    "            tensor = tf.keras.layers.Dropout(rate)(tensor)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def get_model(self):\n",
    "\n",
    "        with tf.name_scope('Network/'):\n",
    "\n",
    "            latent_code = tf.keras.layers.Input(shape=(self.latent_code_dim,))\n",
    "                        \n",
    "            with tf.name_scope('FC_layers'):\n",
    "\n",
    "                fc0 = tf.keras.layers.Dense(self.fc_channels, activation=self.fc_activation)(latent_code)\n",
    "\n",
    "                fc1 = tf.keras.layers.Dense(self.fc_channels, activation=self.fc_activation)(fc0)\n",
    "\n",
    "                fc2 = tf.keras.layers.Dense(self.fc_channels, activation=self.fc_activation)(fc1)\n",
    "\n",
    "                fc2_as_volume = tf.keras.layers.Reshape((1, 1, 1, self.fc_channels))(fc2)\n",
    "\n",
    "            with tf.name_scope('GLO_VoxelDecoder'):\n",
    "\n",
    "                decoder_1 = self.conv_t_block_3d(fc2_as_volume,\n",
    "\n",
    "                                                 num_filters=32,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                decoder_2 = self.conv_t_block_3d(decoder_1,\n",
    "\n",
    "                                                 num_filters=32,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                decoder_3 = self.conv_t_block_3d(decoder_2,\n",
    "\n",
    "                                                 num_filters=32,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                decoder_4 = self.conv_t_block_3d(decoder_3,\n",
    "\n",
    "                                                 num_filters=16,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                decoder_5 = self.conv_t_block_3d(decoder_4,\n",
    "\n",
    "                                                 num_filters=8,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                decoder_6 = self.conv_t_block_3d(decoder_5,\n",
    "\n",
    "                                                 num_filters=4,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                conv_3D_transpose_out = tf.keras.layers.Conv3DTranspose(1,\n",
    "\n",
    "                                                                        self.conv_size,\n",
    "\n",
    "                                                                        strides=2,\n",
    "\n",
    "                                                                        padding='same',\n",
    "\n",
    "                                                                        kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "\n",
    "                                                                        use_bias=False)\n",
    "\n",
    "                volume_out = conv_3D_transpose_out(decoder_6)\n",
    "\n",
    "        return tf.keras.Model(inputs=[latent_code], outputs=[volume_out])\n",
    "\n",
    "    def init_model(self):\n",
    "        self.model = self.get_model()\n",
    "        self.model_backup = self.get_model()\n",
    "\n",
    "        self.latest_epoch = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
    "\n",
    "        init_latent_code = tf.random.normal((self.num_latent_codes, self.latent_code_dim))\n",
    "        self.latent_code_vars = tf.Variable(init_latent_code, trainable=True)\n",
    "\n",
    "        self.trainable_variables = self.model.trainable_variables\n",
    "\n",
    "    def init_optimizer(self):\n",
    "        self.optimizer_network = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_network)\n",
    "        self.optimizer_latent = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_codes)\n",
    "    \n",
    "    def init_losser(self):\n",
    "        self.losser_bce = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    def init_checkpoint(self):\n",
    "        self.checkpoint = tf.train.Checkpoint(\n",
    "            model=self.model,\n",
    "            latent_code_var=self.latent_code_vars,\n",
    "            optimizer_network=self.optimizer_network,\n",
    "            optimizer_latent=self.optimizer_latent,\n",
    "            epoch=self.latest_epoch)\n",
    "\n",
    "        self.manager = tf.train.CheckpointManager(checkpoint=self.checkpoint,\n",
    "                                                  directory=self.checkpoint_dir,\n",
    "                                                  max_to_keep=3)\n",
    "\n",
    "        self.load_checkpoint()\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        latest_checkpoint = self.manager.latest_checkpoint\n",
    "\n",
    "        if latest_checkpoint is not None:\n",
    "            print('Checkpoint {} restored'.format(latest_checkpoint))\n",
    "\n",
    "        self.checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "\n",
    "        for a, b in zip(self.model_backup.variables, self.model.variables):\n",
    "            a.assign(b)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, latent_code_vars, true_voxels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_logits_voxels = self.model(latent_code_vars)\n",
    "            \n",
    "            pred_voxels = tf.sigmoid(pred_logits_voxels)\n",
    "                        \n",
    "            total_loss = self.losser_bce(true_voxels, pred_voxels)\n",
    "        \n",
    "        network_vars = self.trainable_variables\n",
    "                \n",
    "        gradients = tape.gradient(total_loss, network_vars + [latent_code_vars])\n",
    "        \n",
    "        self.optimizer_network.apply_gradients(zip(gradients[:len(network_vars)], network_vars))\n",
    "        self.optimizer_latent.apply_gradients(zip(gradients[len(network_vars):], [latent_code_vars]))\n",
    "                \n",
    "        return total_loss\n",
    "    \n",
    "    def update_latent_code_vars(self, latent_code_vars):\n",
    "        self.latent_code_vars.assign(latent_code_vars)\n",
    "    \n",
    "    def save_models(self, curr_epoch):\n",
    "        self.latest_epoch.assign(curr_epoch)\n",
    "        self.manager.save()\n",
    "\n",
    "    def reset_models(self):\n",
    "        for a, b in zip(self.model.variables, self.model_backup.variables):\n",
    "            a.assign(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_EPOCH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_network = GeometryNetwork(model_hparam)\n",
    "\n",
    "latest_epoch = geom_network.latest_epoch.numpy()\n",
    "\n",
    "for epoch in range(latest_epoch+1, latest_epoch+TRAINING_EPOCH+1):\n",
    "    total_loss = []\n",
    "    \n",
    "    all_latent_code_vars = geom_network.latent_code_vars.numpy()\n",
    "        \n",
    "    pbar = tqdm(data_generator, desc=\"Training EPOCH {}/{}\".format(epoch, latest_epoch+TRAINING_EPOCH))\n",
    "    \n",
    "    for voxel_indexes, true_voxels in pbar:\n",
    "        batch_latent_code_vars = tf.Variable(all_latent_code_vars[voxel_indexes], trainable=True)\n",
    "        \n",
    "        loss = geom_network.train_step(batch_latent_code_vars, np.expand_dims(true_voxels, axis=-1)).numpy()\n",
    "        \n",
    "        all_latent_code_vars[voxel_indexes] = batch_latent_code_vars.numpy()\n",
    "        \n",
    "        total_loss.append(loss)\n",
    "                \n",
    "        pbar.set_postfix({\"Avg Loss\": '{:.5f}'.format(sum(total_loss) / len(total_loss))}) \n",
    "    \n",
    "    geom_network.update_latent_code_vars(all_latent_code_vars)\n",
    "        \n",
    "    print(\"[EPOCH {}] Average Training Loss: {:.5f}\".format(epoch, sum(total_loss) / len(total_loss)))\n",
    "    \n",
    "    geom_network.save_models(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = random.randint(0, len(data_generator))\n",
    "\n",
    "true_voxels_coords = np.load(os.path.join(DATASET_DIR_PTH, str(test_target)+'.npy'))\n",
    "\n",
    "true_voxels = np.zeros((128, 128, 128), dtype=np.float32)\n",
    "true_voxels[tuple(true_voxels_coords.T)] = 1.0\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.set_aspect('equal')\n",
    "ax.voxels(true_voxels)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "latent_codes = tf.expand_dims(geom_network.latent_code_vars[test_target], axis=0)\n",
    "\n",
    "pred_logits_voxels = geom_network.model(latent_codes)\n",
    "pred_voxels = tf.sigmoid(pred_logits_voxels)\n",
    "pred_voxels = tf.cast(tf.math.greater_equal(pred_voxels, 0.5), tf.float32)\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.set_aspect('equal')\n",
    "ax.voxels(pred_voxels[0, :, :, :].numpy().reshape((pred_voxels.shape[1:4])))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
