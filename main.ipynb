{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_1d(near, far, num_samples, name=\"stratified_1d\"):\n",
    "    with tf.name_scope(name):\n",
    "        near = tf.convert_to_tensor(near)\n",
    "        far = tf.convert_to_tensor(far)\n",
    "\n",
    "        bin_borders = tf.linspace(0.0, 1.0, num_samples + 1, axis=-1)\n",
    "        bin_below = bin_borders[..., :-1]\n",
    "        bin_above = bin_borders[..., 1:]\n",
    "        target_shape = tf.concat([tf.shape(near), [num_samples]], axis=-1)\n",
    "        random_point_in_bin = tf.random.uniform(target_shape)\n",
    "        z_values = bin_below + (bin_above - bin_below) * random_point_in_bin\n",
    "        z_values = (tf.expand_dims(near, -1) * (1. - z_values) +\n",
    "                    tf.expand_dims(far, -1) * z_values)\n",
    "\n",
    "        return z_values\n",
    "\n",
    "\n",
    "def points_from_z_values(ray_org, ray_dir, z_values):\n",
    "    points3d = (tf.expand_dims(ray_dir, axis=-2) *\n",
    "                tf.expand_dims(z_values, axis=-1))\n",
    "    points3d = tf.expand_dims(ray_org, -2) + points3d\n",
    "    return points3d\n",
    "\n",
    "\n",
    "def sample_1d(ray_org, ray_dir, near, far, n_samples, name=\"sample_1d\"):\n",
    "    with tf.name_scope(name):\n",
    "        ray_org = tf.convert_to_tensor(ray_org)\n",
    "        ray_dir = tf.convert_to_tensor(ray_dir)\n",
    "\n",
    "        near = tf.convert_to_tensor(near) * tf.ones((1,))\n",
    "        far = tf.convert_to_tensor(far) * tf.ones((1,))\n",
    "\n",
    "        near = near * tf.ones(tf.shape(ray_org)[:-1])\n",
    "        far = far * tf.ones(tf.shape(ray_org)[:-1])\n",
    "\n",
    "        random_z_values = stratified_1d(near, far, n_samples)\n",
    "        points3d = points_from_z_values(ray_org, ray_dir, random_z_values)\n",
    "\n",
    "        return points3d\n",
    "\n",
    "\n",
    "def match_intermediate_batch_dimensions(tensor1, tensor2):\n",
    "    shape1 = tf.shape(tensor1)\n",
    "    shape2 = tf.shape(tensor2)\n",
    "\n",
    "    shape_diff = len(shape2) - len(shape1)\n",
    "    new_shape = tf.concat([[shape1[0]], [1]*shape_diff, [shape1[-1]]], axis=-1)\n",
    "    target_shape = tf.concat([shape2[:-1], [shape1[-1]]], axis=-1)\n",
    "\n",
    "    return tf.broadcast_to(tf.reshape(tensor1, new_shape), target_shape)\n",
    "\n",
    "\n",
    "def trilinear_interpolate(grid_3d, sampling_points, name=\"trilinear_interpolate\"):\n",
    "    with tf.name_scope(name):\n",
    "        grid_3d = tf.convert_to_tensor(value=grid_3d)\n",
    "        sampling_points = tf.convert_to_tensor(value=sampling_points)\n",
    "\n",
    "        voxel_cube_shape = tf.shape(input=grid_3d)[-4:-1]\n",
    "        sampling_points.set_shape(sampling_points.shape)\n",
    "        batch_dims = tf.shape(input=sampling_points)[:-2]\n",
    "        num_points = tf.shape(input=sampling_points)[-2]\n",
    "\n",
    "        bottom_left = tf.floor(sampling_points)\n",
    "        top_right = bottom_left + 1\n",
    "        bottom_left_index = tf.cast(bottom_left, tf.int32)\n",
    "        top_right_index = tf.cast(top_right, tf.int32)\n",
    "        x0_index, y0_index, z0_index = tf.unstack(bottom_left_index, axis=-1)\n",
    "        x1_index, y1_index, z1_index = tf.unstack(top_right_index, axis=-1)\n",
    "        index_x = tf.concat([x0_index, x1_index, x0_index, x1_index,\n",
    "                             x0_index, x1_index, x0_index, x1_index], axis=-1)\n",
    "        index_y = tf.concat([y0_index, y0_index, y1_index, y1_index,\n",
    "                             y0_index, y0_index, y1_index, y1_index], axis=-1)\n",
    "        index_z = tf.concat([z0_index, z0_index, z0_index, z0_index,\n",
    "                             z1_index, z1_index, z1_index, z1_index], axis=-1)\n",
    "        indices = tf.stack([index_x, index_y, index_z], axis=-1)\n",
    "        clip_value = tf.convert_to_tensor(\n",
    "            value=[voxel_cube_shape - 1], dtype=indices.dtype)\n",
    "        indices = tf.clip_by_value(indices, 0, clip_value)\n",
    "        content = tf.gather_nd(\n",
    "            params=grid_3d, indices=indices, batch_dims=tf.size(input=batch_dims))\n",
    "        distance_to_bottom_left = sampling_points - bottom_left\n",
    "        distance_to_top_right = top_right - sampling_points\n",
    "        x_x0, y_y0, z_z0 = tf.unstack(distance_to_bottom_left, axis=-1)\n",
    "        x1_x, y1_y, z1_z = tf.unstack(distance_to_top_right, axis=-1)\n",
    "        weights_x = tf.concat([x1_x, x_x0, x1_x, x_x0,\n",
    "                               x1_x, x_x0, x1_x, x_x0], axis=-1)\n",
    "        weights_y = tf.concat([y1_y, y1_y, y_y0, y_y0,\n",
    "                               y1_y, y1_y, y_y0, y_y0], axis=-1)\n",
    "        weights_z = tf.concat([z1_z, z1_z, z1_z, z1_z,\n",
    "                               z_z0, z_z0, z_z0, z_z0], axis=-1)\n",
    "        weights = tf.expand_dims(weights_x * weights_y * weights_z, axis=-1)\n",
    "\n",
    "        interpolated_values = weights * content\n",
    "        return tf.add_n(tf.split(interpolated_values, [num_points] * 8, -2))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def ray_sample_voxel_grid(ray_points, voxels, w2v_alpha, w2v_beta):\n",
    "    w2v_alpha = match_intermediate_batch_dimensions(w2v_alpha, ray_points)\n",
    "    w2v_beta = match_intermediate_batch_dimensions(w2v_beta, ray_points)\n",
    "    rays = w2v_alpha*ray_points + w2v_beta\n",
    "\n",
    "    batch_size = tf.shape(voxels)[0]\n",
    "    channels = tf.shape(voxels)[-1]\n",
    "\n",
    "    target_shape = tf.concat([tf.shape(rays)[:-1], [channels]], axis=-1)\n",
    "\n",
    "    rays = tf.reshape(rays, [batch_size, -1, 3])\n",
    "    features_alpha = trilinear_interpolate(voxels, rays)\n",
    "\n",
    "    return tf.reshape(features_alpha, target_shape)\n",
    "\n",
    "\n",
    "def compute_density(density_values, distances, name=None):\n",
    "    with tf.compat.v1.name_scope(name, \"ray_density\", [density_values, distances]):\n",
    "        density_values = tf.convert_to_tensor(value=density_values)\n",
    "        distances = tf.convert_to_tensor(value=distances)\n",
    "        distances = tf.expand_dims(distances, -1)\n",
    "\n",
    "        alpha = 1. - tf.exp(-density_values * distances)\n",
    "        alpha = tf.squeeze(alpha, -1)\n",
    "        ray_sample_weights = alpha * \\\n",
    "            tf.math.cumprod(1. - alpha + 1e-10, -1, exclusive=True)\n",
    "        ray_alpha = tf.expand_dims(\n",
    "            tf.reduce_sum(ray_sample_weights, -1), axis=-1)\n",
    "        return ray_alpha\n",
    "\n",
    "\n",
    "def l2_loss(prediction, target, weights=1.0):\n",
    "    assert prediction.shape == target.shape, \"Shape dims should be the same.\"\n",
    "    return tf.reduce_mean(weights * tf.square(target - prediction))\n",
    "\n",
    "\n",
    "def select_eps_for_division(dtype):\n",
    "    return 10.0 * np.finfo(dtype.as_numpy_dtype).tiny\n",
    "\n",
    "\n",
    "def assert_no_infs_or_nans(tensor, name='assert_no_infs_or_nans'):\n",
    "    with tf.name_scope(name):\n",
    "        tensor = tf.convert_to_tensor(value=tensor)\n",
    "\n",
    "        assert_ops = (tf.debugging.check_numerics(\n",
    "            tensor, message='Inf or NaN detected.'),)\n",
    "        with tf.control_dependencies(assert_ops):\n",
    "            return tf.identity(tensor)\n",
    "\n",
    "\n",
    "def nonzero_sign(x, name='nonzero_sign'):\n",
    "    with tf.name_scope(name):\n",
    "        x = tf.convert_to_tensor(value=x)\n",
    "\n",
    "        one = tf.ones_like(x)\n",
    "        return tf.where(tf.greater_equal(x, 0.0), one, -one)\n",
    "\n",
    "\n",
    "def safe_signed_div(a, b, eps=None, name='safe_signed_div'):\n",
    "    with tf.name_scope(name):\n",
    "        a = tf.convert_to_tensor(value=a)\n",
    "        b = tf.convert_to_tensor(value=b)\n",
    "\n",
    "        if eps is None:\n",
    "            eps = select_eps_for_division(b.dtype)\n",
    "\n",
    "        eps = tf.convert_to_tensor(value=eps, dtype=b.dtype)\n",
    "\n",
    "        return assert_no_infs_or_nans(a / (b + nonzero_sign(b) * eps))\n",
    "\n",
    "\n",
    "def ray(point_2d, focal, principal_point, name=\"perspective_ray\"):\n",
    "    with tf.name_scope(name):\n",
    "        point_2d = tf.convert_to_tensor(value=point_2d)\n",
    "        focal = tf.convert_to_tensor(value=focal)\n",
    "        principal_point = tf.convert_to_tensor(value=principal_point)\n",
    "\n",
    "        point_2d -= principal_point\n",
    "        point_2d = safe_signed_div(point_2d, focal)\n",
    "        padding = [[0, 0] for _ in point_2d.shape]\n",
    "        padding[-1][-1] = 1\n",
    "\n",
    "        return tf.pad(tensor=point_2d, paddings=padding, mode=\"CONSTANT\", constant_values=1.0)\n",
    "\n",
    "\n",
    "def perspective_random_rays(focal, principal_point, height, width, n_rays, margin=0, name=\"random_rays\"):\n",
    "    with tf.name_scope(name):\n",
    "        focal = tf.convert_to_tensor(value=focal)\n",
    "        principal_point = tf.convert_to_tensor(value=principal_point)\n",
    "        batch_dims = tf.shape(focal)[:-1]\n",
    "        target_shape = tf.concat([batch_dims, [n_rays]], axis=0)\n",
    "        random_x = tf.random.uniform(\n",
    "            target_shape, minval=margin, maxval=width - margin, dtype=tf.int32)\n",
    "        random_y = tf.random.uniform(\n",
    "            target_shape, minval=margin, maxval=height - margin, dtype=tf.int32)\n",
    "        pixels = tf.cast(tf.stack((random_x, random_y), axis=-1), tf.float32)\n",
    "        rays = ray(pixels, tf.expand_dims(focal, -2),\n",
    "                   tf.expand_dims(principal_point, -2))\n",
    "        return rays, tf.cast(pixels, tf.int32)\n",
    "\n",
    "\n",
    "def build_matrix_from_sines_and_cosines(sin_angles, cos_angles):\n",
    "    sin_angles.shape.assert_is_compatible_with(cos_angles.shape)\n",
    "\n",
    "    sx, sy, sz = tf.unstack(sin_angles, axis=-1)\n",
    "    cx, cy, cz = tf.unstack(cos_angles, axis=-1)\n",
    "    m00 = cy * cz\n",
    "    m01 = (sx * sy * cz) - (cx * sz)\n",
    "    m02 = (cx * sy * cz) + (sx * sz)\n",
    "    m10 = cy * sz\n",
    "    m11 = (sx * sy * sz) + (cx * cz)\n",
    "    m12 = (cx * sy * sz) - (sx * cz)\n",
    "    m20 = -sy\n",
    "    m21 = sx * cy\n",
    "    m22 = cx * cy\n",
    "    matrix = tf.stack((m00, m01, m02,\n",
    "                       m10, m11, m12,\n",
    "                       m20, m21, m22),\n",
    "                      axis=-1)  # pyformat: disable\n",
    "    output_shape = tf.concat(\n",
    "        (tf.shape(input=sin_angles)[:-1], (3, 3)), axis=-1)\n",
    "    return tf.reshape(matrix, shape=output_shape)\n",
    "\n",
    "\n",
    "def rotation_matrix_3d_from_euler(angles, name=\"rotation_matrix_3d_from_euler\"):\n",
    "    with tf.name_scope(name):\n",
    "        angles = tf.convert_to_tensor(value=angles)\n",
    "\n",
    "        sin_angles = tf.sin(angles)\n",
    "        cos_angles = tf.cos(angles)\n",
    "        return build_matrix_from_sines_and_cosines(sin_angles, cos_angles)\n",
    "\n",
    "\n",
    "def change_coordinate_system(points3d, rotations=(0., 0., 0.), scale=(1., 1., 1.), name=\"change_coordinate_system\"):\n",
    "    with tf.name_scope(name):\n",
    "        points3d = tf.convert_to_tensor(points3d)\n",
    "        rotation = tf.convert_to_tensor(rotations)\n",
    "        scale = tf.convert_to_tensor(scale)\n",
    "\n",
    "        rotation_matrix = rotation_matrix_3d_from_euler(rotation)\n",
    "        scaling_matrix = scale*tf.eye(3, 3)\n",
    "\n",
    "        target_shape = [1]*(len(points3d.get_shape().as_list()) - 2) + [3, 3]\n",
    "        transformation = tf.matmul(scaling_matrix, rotation_matrix)\n",
    "        transformation = tf.reshape(transformation, target_shape)\n",
    "\n",
    "        return tf.linalg.matrix_transpose(tf.matmul(transformation, tf.linalg.matrix_transpose(points3d)))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def _move_in_front_of_camera(points3d, rotation_matrix, translation_vector):\n",
    "    points3d = tf.convert_to_tensor(value=points3d)\n",
    "    rotation_matrix = tf.convert_to_tensor(value=rotation_matrix)\n",
    "    translation_vector = tf.convert_to_tensor(value=translation_vector)\n",
    "\n",
    "    points3d_corrected = tf.linalg.matrix_transpose(\n",
    "        points3d) + translation_vector\n",
    "    rotation_matrix_t = -tf.linalg.matrix_transpose(rotation_matrix)\n",
    "    points3d_world = tf.matmul(rotation_matrix_t, points3d_corrected)\n",
    "\n",
    "    return tf.linalg.matrix_transpose(points3d_world)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def camera_rays_from_extrinsics(rays, rotation_matrix, translation_vector):\n",
    "    rays_org = _move_in_front_of_camera(tf.zeros_like(\n",
    "        rays), rotation_matrix, translation_vector)\n",
    "    rays_dir_ = _move_in_front_of_camera(\n",
    "        rays, rotation_matrix, 0 * translation_vector)\n",
    "    rays_dir = rays_dir_/tf.norm(rays_dir_, axis=-1, keepdims=True)\n",
    "    return rays_org, rays_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam = {\n",
    "    'model_latent_code_dim': 256,\n",
    "    'model_fc_channels': 512,\n",
    "    'model_fc_activation': 'relu',\n",
    "    'model_norm_3d': 'batchnorm',\n",
    "    'model_conv_size': 4,\n",
    "    'model_num_latent_codes': 4371,\n",
    "    'model_learning_rate_network': 1e-4,\n",
    "    'model_learning_rate_codes': 1e-4,\n",
    "    'model_checkpoint_dir': './ckpt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeometryNetwork:\n",
    "    def __init__(self, hparam):\n",
    "        self.latent_code_dim = hparam['model_latent_code_dim']\n",
    "        self.fc_channels = hparam['model_fc_channels']\n",
    "        self.fc_activation = hparam['model_fc_activation']\n",
    "        self.norm_3d = hparam['model_norm_3d']\n",
    "        self.conv_size = hparam['model_conv_size']\n",
    "        self.num_latent_codes = hparam['model_num_latent_codes']\n",
    "        self.learning_rate_network = hparam['model_learning_rate_network']\n",
    "        self.learning_rate_codes = hparam['model_learning_rate_codes']\n",
    "        self.checkpoint_dir = hparam['model_checkpoint_dir']\n",
    "\n",
    "        self.mask_voxels = self.get_mask_voxels()\n",
    "\n",
    "        self.init_model()\n",
    "        self.init_optimizer()\n",
    "        self.init_checkpoint()\n",
    "\n",
    "    def get_mask_voxels(self, shape=(1, 128, 128, 128, 1), dtype=np.float32):\n",
    "        voxels = np.ones(shape, dtype=dtype)\n",
    "        voxels[:, [0, -1], :, :, :] = 0\n",
    "        voxels[:, :, [0, -1], :, :] = 0\n",
    "        voxels[:, :, :, [0, -1], :] = 0\n",
    "        return tf.convert_to_tensor(voxels)\n",
    "\n",
    "    def norm_layer(self, tensor, normalization):\n",
    "        if normalization and normalization.lower() == 'batchnorm':\n",
    "            tensor = tf.keras.layers.BatchNormalization()(tensor)\n",
    "        return tensor\n",
    "\n",
    "    def conv_t_block_3d(self, tensor, num_filters, size, strides,\n",
    "                        normalization=None, dropout=False,\n",
    "                        alpha_lrelu=0.2, relu=True, rate=0.7):\n",
    "        conv_3D_transpose = tf.keras.layers.Conv3DTranspose(num_filters,\n",
    "                                                            size,\n",
    "                                                            strides=strides,\n",
    "                                                            padding='same',\n",
    "                                                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "                                                            use_bias=False)\n",
    "\n",
    "        tensor = conv_3D_transpose(tensor)\n",
    "\n",
    "        tensor = self.norm_layer(tensor, normalization)\n",
    "\n",
    "        if relu:\n",
    "            tensor = tf.keras.layers.LeakyReLU(alpha=alpha_lrelu)(tensor)\n",
    "\n",
    "        if dropout:\n",
    "            tensor = tf.keras.layers.Dropout(rate)(tensor)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def get_model(self):\n",
    "\n",
    "        with tf.name_scope('Network/'):\n",
    "\n",
    "            latent_code = tf.keras.layers.Input(shape=(self.latent_code_dim,))\n",
    "\n",
    "            with tf.name_scope('FC_layers'):\n",
    "\n",
    "                fc0 = tf.keras.layers.Dense(self.fc_channels, activation=self.fc_activation)(latent_code)\n",
    "\n",
    "                fc1 = tf.keras.layers.Dense(self.fc_channels, activation=self.fc_activation)(fc0)\n",
    "\n",
    "                fc2 = tf.keras.layers.Dense(self.fc_channels, activation=self.fc_activation)(fc1)\n",
    "\n",
    "                fc2_as_volume = tf.keras.layers.Reshape((1, 1, 1, self.fc_channels))(fc2)\n",
    "\n",
    "            with tf.name_scope('GLO_VoxelDecoder'):\n",
    "\n",
    "                decoder_1 = self.conv_t_block_3d(fc2_as_volume,\n",
    "\n",
    "                                                 num_filters=32,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                decoder_2 = self.conv_t_block_3d(decoder_1,\n",
    "\n",
    "                                                 num_filters=32,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                decoder_3 = self.conv_t_block_3d(decoder_2,\n",
    "\n",
    "                                                 num_filters=32,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                decoder_4 = self.conv_t_block_3d(decoder_3,\n",
    "\n",
    "                                                 num_filters=16,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                decoder_5 = self.conv_t_block_3d(decoder_4,\n",
    "\n",
    "                                                 num_filters=8,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                decoder_6 = self.conv_t_block_3d(decoder_5,\n",
    "\n",
    "                                                 num_filters=4,\n",
    "\n",
    "                                                 size=self.conv_size,\n",
    "\n",
    "                                                 strides=2,\n",
    "\n",
    "                                                 normalization=self.norm_3d)\n",
    "\n",
    "                conv_3D_transpose_out = tf.keras.layers.Conv3DTranspose(1,\n",
    "\n",
    "                                                                        self.conv_size,\n",
    "\n",
    "                                                                        strides=2,\n",
    "\n",
    "                                                                        padding='same',\n",
    "\n",
    "                                                                        kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "\n",
    "                                                                        use_bias=False)\n",
    "\n",
    "                volume_out = conv_3D_transpose_out(decoder_6)\n",
    "\n",
    "        return tf.keras.Model(inputs=[latent_code], outputs=[volume_out])\n",
    "\n",
    "    def init_model(self):\n",
    "        self.model = self.get_model()\n",
    "        self.model_backup = self.get_model()\n",
    "\n",
    "        self.latest_epoch = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
    "        self.global_step = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
    "\n",
    "        init_latent_code = tf.random.normal(\n",
    "            (self.num_latent_codes, self.latent_code_dim))\n",
    "        self.latent_code_vars = tf.Variable(init_latent_code, trainable=True)\n",
    "\n",
    "        self.trainable_variables = self.model.trainable_variables\n",
    "\n",
    "    def init_optimizer(self):\n",
    "        self.optimizer_network = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.learning_rate_network)\n",
    "        self.optimizer_latent = tf.keras.optimizers.Adam(\n",
    "            learning_rate=self.learning_rate_codes)\n",
    "    \n",
    "    def init_checkpoint(self):\n",
    "        self.summary_writer = tf.summary.create_file_writer(self.checkpoint_dir)\n",
    "        \n",
    "        self.checkpoint = tf.train.Checkpoint(\n",
    "            model=self.model,\n",
    "            latent_code_var=self.latent_code_vars,\n",
    "            optimizer_network=self.optimizer_network,\n",
    "            optimizer_latent=self.optimizer_latent,\n",
    "            epoch=self.latest_epoch,\n",
    "            global_step=self.global_step)\n",
    "        \n",
    "        self.manager = tf.train.CheckpointManager(checkpoint=self.checkpoint,\n",
    "                                                  directory=self.checkpoint_dir,\n",
    "                                                  max_to_keep=2)\n",
    "        \n",
    "        self.load_checkpoint()\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        latest_checkpoint = self.manager.latest_checkpoint\n",
    "        \n",
    "        if latest_checkpoint is not None:\n",
    "            print('Checkpoint {} restored'.format(latest_checkpoint))\n",
    "            \n",
    "        self.checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "        \n",
    "        for a, b in zip(self.model_backup.variables, self.model.variables):\n",
    "            a.assign(b)\n",
    "        else:\n",
    "            print('No checkpoint was restored.')\n",
    "\n",
    "    def reset_models(self):\n",
    "        for a, b in zip(self.model.variables, self.model_backup.variables):\n",
    "            a.assign(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_for_mask(geometry_network,\n",
    "                      mask,\n",
    "                      focal,\n",
    "                      principal_point,\n",
    "                      rotation_matrix,\n",
    "                      translation_vector,\n",
    "                      w2v_alpha,\n",
    "                      w2v_beta,\n",
    "                      near=1.25,\n",
    "                      far=3.5,\n",
    "                      n_samples=128,\n",
    "                      density=20,\n",
    "                      mirror_weight=1.0,\n",
    "                      n_iter=100,\n",
    "                      n_rays=1024):\n",
    "\n",
    "    height, width = mask.shape[-3], mask.shape[-2]\n",
    "\n",
    "    voxel_code_var = tf.reduce_mean(geometry_network.latent_code_vars, axis=0, keepdims=True)\n",
    "    voxel_code_var = tf.Variable(voxel_code_var, trainable=True)\n",
    "\n",
    "    network_vars = geometry_network.trainable_variables\n",
    "\n",
    "    @tf.function\n",
    "    def opt_step(r_org, r_dir, gt_a):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_logits_voxels = geometry_network.model(voxel_code_var)\n",
    "            voxels = tf.sigmoid(pred_logits_voxels) * geometry_network.mask_voxels\n",
    "\n",
    "            ray_points_coarse = sample_1d(r_org, r_dir, near=near, far=far, n_samples=n_samples)\n",
    "\n",
    "            voxel_values = ray_sample_voxel_grid(ray_points_coarse, voxels, w2v_alpha, w2v_beta)\n",
    "            silhouettes = compute_density(voxel_values, density*tf.ones_like(voxel_values[..., 0]))\n",
    "\n",
    "            silhouette_loss = l2_loss(silhouettes, gt_a)\n",
    "            mirror_voxel_loss = l2_loss(voxels, tf.reverse(voxels, [1]))\n",
    "            total_loss = silhouette_loss + mirror_weight*mirror_voxel_loss\n",
    "\n",
    "        gradients = tape.gradient(total_loss, network_vars + [voxel_code_var])\n",
    "        geometry_network.optimizer_network.apply_gradients(zip(gradients[:len(network_vars)], network_vars))\n",
    "        geometry_network.optimizer_latent.apply_gradients(zip(gradients[len(network_vars):], [voxel_code_var]))\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    for it in range(n_iter):\n",
    "        random_rays, random_pixels_xy = perspective_random_rays(focal, principal_point, height, width, n_rays)\n",
    "        random_rays = change_coordinate_system(random_rays, (0., math.pi, math.pi), (-1., 1., 1.))\n",
    "        rays_org, rays_dir = camera_rays_from_extrinsics(random_rays, rotation_matrix, translation_vector)\n",
    "\n",
    "        random_pixels_yx = tf.reverse(random_pixels_xy, axis=[-1])\n",
    "        random_pixels_yx = tf.cast(random_pixels_yx, tf.int32)\n",
    "        pixels = tf.gather_nd(mask, random_pixels_yx, batch_dims=1)\n",
    "        loss = opt_step(rays_org, rays_dir, pixels)\n",
    "\n",
    "        print('Iter {:>2d} loss: {:.5f}'.format(it, loss))\n",
    "\n",
    "    return voxel_code_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTest(tf.test.TestCase):\n",
    "    def test_model_training(self):\n",
    "        batch_size = 10\n",
    "        latent_code_dim = 256\n",
    "\n",
    "        geom_network = GeometryNetwork(hparam)\n",
    "\n",
    "        latent_codes = tf.zeros((batch_size, latent_code_dim))\n",
    "        pred_logits_voxels = geom_network.model(latent_codes)\n",
    "        pred_voxels = tf.sigmoid(pred_logits_voxels)\n",
    "\n",
    "        self.assertAllInRange(pred_voxels, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationTest(tf.test.TestCase):\n",
    "    def test_optimization(self):\n",
    "        geom_network = GeometryNetwork(hparam)\n",
    "\n",
    "        mask = tf.ones((1, 128, 128, 1))\n",
    "\n",
    "        latent_code = optimize_for_mask(\n",
    "            geom_network,\n",
    "            mask,\n",
    "            focal=tf.ones((1, 2)),\n",
    "            principal_point=tf.ones((1, 2)),\n",
    "            rotation_matrix=tf.expand_dims(tf.eye(3, 3), 0),\n",
    "            translation_vector=tf.ones((1, 3, 1)),\n",
    "            w2v_alpha=tf.ones((1, 3)),\n",
    "            w2v_beta=tf.ones((1, 3)),\n",
    "            near=1.25,\n",
    "            far=3.5,\n",
    "            n_samples=128,\n",
    "            density=1,\n",
    "            mirror_weight=50.0,\n",
    "            n_iter=10,\n",
    "            n_rays=2024\n",
    "        )\n",
    "\n",
    "        pred_logits_voxels = geom_network.model(latent_code)\n",
    "        pred_voxels = tf.sigmoid(pred_logits_voxels) * geom_network.mask_voxels\n",
    "\n",
    "        self.assertAllInRange(pred_voxels, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelTest().test_model_training()\n",
    "OptimizationTest().test_optimization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
