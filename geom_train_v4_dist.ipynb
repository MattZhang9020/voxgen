{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_data_to_dense(voxel_data, voxel_map_shape=(128, 128, 128)):\n",
    "    dense_voxel_maps = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    \n",
    "    for voxel_coords_tensor in voxel_data:\n",
    "        value_map = tf.ones(tf.shape(voxel_coords_tensor)[:-1], dtype=tf.float32)\n",
    "\n",
    "        voxel_map = tf.zeros(voxel_map_shape, dtype=tf.float32)\n",
    "        voxel_map = tf.tensor_scatter_nd_add(voxel_map, voxel_coords_tensor, value_map)\n",
    "\n",
    "        dense_voxel_maps = dense_voxel_maps.write(dense_voxel_maps.size(), voxel_map)\n",
    "        \n",
    "    return tf.expand_dims(dense_voxel_maps.stack(), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, dataset_dir_pth, each_chair_parts_count_pth, objs_count=None, batch_size=4):\n",
    "        self.dataset_dir_pth = dataset_dir_pth\n",
    "\n",
    "        self.each_chair_parts_count = np.load(each_chair_parts_count_pth)[:objs_count]\n",
    "        self.num_objts = len(self.each_chair_parts_count)\n",
    "\n",
    "        self.data_names = np.array(sorted_alphanumeric(os.listdir(self.dataset_dir_pth)), dtype=str)[:self._get_total_parts_size()]\n",
    "        self.num_parts = len(self.data_names)\n",
    "\n",
    "        self.batch_szie = batch_size\n",
    "\n",
    "        self.part_voxels_coords = self._load_voxel_data()\n",
    "\n",
    "    def _get_total_parts_size(self):\n",
    "        count = 0\n",
    "\n",
    "        if self.num_objts == None:\n",
    "            return None\n",
    "\n",
    "        for i in range(self.num_objts):\n",
    "            count += self.each_chair_parts_count[i]\n",
    "\n",
    "        return count\n",
    "\n",
    "    def _load_voxel_data(self):\n",
    "        print('Trying to load {} objects with total {} parts.'.format(self.num_objts, self.num_parts))\n",
    "        \n",
    "        def load_data(data_pth):\n",
    "            return np.load(data_pth)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            data_paths = [os.path.join(self.dataset_dir_pth, data_name) for data_name in self.data_names]\n",
    "            part_voxels_coords = list(tqdm(executor.map(load_data, data_paths), total=len(data_paths)))\n",
    "\n",
    "        return part_voxels_coords\n",
    "    \n",
    "    def get_tf_dataset(self):\n",
    "        part_indices = tf.range(self.num_parts)\n",
    "        part_voxels_coords = tf.RaggedTensor.from_row_lengths(tf.concat(self.part_voxels_coords, axis=0), row_lengths=[a.shape[0] for a in self.part_voxels_coords])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((part_indices, part_voxels_coords)).batch(self.batch_szie, drop_remainder=False)\n",
    "        return dataset\n",
    "\n",
    "    def reset_index(self):\n",
    "        self.curr_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EACH_CHAIR_PARTS_COUNT_PTH = \".\\\\dataset\\\\each_chair_parts_count.npy\"\n",
    "DATASET_DIR_PTH = \".\\\\dataset\\\\chair_voxel_data\"\n",
    "\n",
    "LOAD_OBJS_COUNT = None\n",
    "VOXEL_MAP_SHAPE = (128, 128, 128)\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 8\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(dataset_dir_pth=DATASET_DIR_PTH,\n",
    "                               each_chair_parts_count_pth=EACH_CHAIR_PARTS_COUNT_PTH,\n",
    "                               objs_count=LOAD_OBJS_COUNT,\n",
    "                               batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "dataset = data_generator.get_tf_dataset()\n",
    "\n",
    "dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "\n",
    "dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartNetwork:\n",
    "    def __init__(self, hparam):\n",
    "        self.latent_code_dim = hparam['model_latent_code_dim']\n",
    "\n",
    "        self.fc_channels = hparam['model_fc_channels']\n",
    "\n",
    "        self.conv_size = hparam['model_conv_size']\n",
    "\n",
    "        self.num_latent_codes_parts = hparam['model_num_latent_codes_parts']\n",
    "\n",
    "        self.learning_rate_network = hparam['model_learning_rate_network']\n",
    "        self.learning_rate_codes = hparam['model_learning_rate_codes']\n",
    "\n",
    "        self.model_voxel_map_shape = hparam['model_voxel_map_shape']\n",
    "\n",
    "        self.checkpoint_dir = hparam['model_checkpoint_dir']\n",
    "\n",
    "        self.ramdom_projection_num = hparam['modelramdom_projection_num']\n",
    "        \n",
    "        self.trained_epoch = tf.Variable(0)\n",
    "\n",
    "        self._init_model()\n",
    "        self._init_optimizer()\n",
    "        self._init_losser()\n",
    "        self._init_checkpoint()\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.part_generator = self._get_generator()\n",
    "\n",
    "        init_latent_code_parts = tf.random.normal((self.num_latent_codes_parts, self.latent_code_dim))\n",
    "        self.latent_code_vars_parts = tf.Variable(init_latent_code_parts, trainable=True)\n",
    "\n",
    "        self.part_generator_trainable_variables = self.part_generator.trainable_variables\n",
    "\n",
    "    def _get_generator(self):\n",
    "        with mirrored_strategy.scope():\n",
    "            \n",
    "            with tf.name_scope('Network/'):\n",
    "\n",
    "                latent_code = tf.keras.layers.Input(shape=(self.latent_code_dim,))\n",
    "\n",
    "                with tf.name_scope('FC_layers'):\n",
    "\n",
    "                    fc0 = tf.keras.layers.Dense(self.fc_channels, activation='relu')(latent_code)\n",
    "\n",
    "                    fc1 = tf.keras.layers.Dense(self.fc_channels, activation='relu')(fc0)\n",
    "\n",
    "                    fc2 = tf.keras.layers.Dense(self.fc_channels, activation='relu')(fc1)\n",
    "\n",
    "                    fc2_as_volume = tf.keras.layers.Reshape((1, 1, 1, self.fc_channels))(fc2)\n",
    "\n",
    "                with tf.name_scope('GLO_VoxelDecoder'):\n",
    "\n",
    "                    decoder_1 = self._conv_t_block_3d(fc2_as_volume, num_filters=32, size=self.conv_size, strides=2)\n",
    "\n",
    "                    decoder_2 = self._conv_t_block_3d(decoder_1, num_filters=32, size=self.conv_size, strides=2)\n",
    "\n",
    "                    decoder_3 = self._conv_t_block_3d(decoder_2, num_filters=32, size=self.conv_size, strides=2)\n",
    "\n",
    "                    decoder_4 = self._conv_t_block_3d(decoder_3, num_filters=16, size=self.conv_size, strides=2)\n",
    "\n",
    "                    decoder_5 = self._conv_t_block_3d(decoder_4, num_filters=8, size=self.conv_size, strides=2)\n",
    "\n",
    "                    decoder_6 = self._conv_t_block_3d(decoder_5, num_filters=4, size=self.conv_size, strides=2)\n",
    "\n",
    "                    volume_out = self._conv_t_block_3d(decoder_6, num_filters=1, size=self.conv_size, strides=2, output_mode=True)\n",
    "\n",
    "            model = tf.keras.Model(inputs=[latent_code], outputs=[volume_out])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _conv_t_block_3d(self, tensor, num_filters, size, strides, alpha_lrelu=0.2, output_mode=False):\n",
    "        conv_3D_transpose = tf.keras.layers.Conv3DTranspose(\n",
    "            filters=num_filters,\n",
    "            kernel_size=size,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "            use_bias=False\n",
    "        )\n",
    "\n",
    "        tensor = conv_3D_transpose(tensor)\n",
    "\n",
    "        if output_mode:\n",
    "            return tensor\n",
    "\n",
    "        tensor = tf.keras.layers.BatchNormalization()(tensor)\n",
    "\n",
    "        tensor = tf.keras.layers.LeakyReLU(alpha=alpha_lrelu)(tensor)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def _init_optimizer(self):\n",
    "        with mirrored_strategy.scope():\n",
    "            self.optimizer_part_generator = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_network)\n",
    "            self.optimizer_latent_for_parts = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_codes)\n",
    "\n",
    "    def _init_losser(self):\n",
    "        self.losser_bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "    def _init_checkpoint(self):\n",
    "        self.checkpoint = tf.train.Checkpoint(\n",
    "            part_generator=self.part_generator,\n",
    "            latent_code_vars_parts=self.latent_code_vars_parts,\n",
    "            optimizer_part_generator=self.optimizer_part_generator,\n",
    "            optimizer_latent_for_parts=self.optimizer_latent_for_parts,\n",
    "            trained_epoch=self.trained_epoch\n",
    "        )\n",
    "\n",
    "        self.manager = tf.train.CheckpointManager(checkpoint=self.checkpoint,\n",
    "                                                  directory=self.checkpoint_dir,\n",
    "                                                  max_to_keep=1)\n",
    "\n",
    "        self._load_checkpoint()\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        latest_checkpoint = self.manager.latest_checkpoint\n",
    "\n",
    "        if latest_checkpoint is not None:\n",
    "            print('Checkpoint {} restored'.format(latest_checkpoint))\n",
    "        else:\n",
    "            print('No checkpoint was restored.')\n",
    "\n",
    "        self.checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_parts(self, latent_code_vars, true_voxels_part):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_logits_voxels = self.part_generator(latent_code_vars)\n",
    "\n",
    "            pred_voxels_part = tf.sigmoid(pred_logits_voxels)\n",
    "\n",
    "            loss = self.losser_bce(true_voxels_part, pred_voxels_part)\n",
    "            loss = tf.nn.compute_average_loss(loss)\n",
    "\n",
    "            model_losses = self.part_generator.losses\n",
    "            if model_losses:\n",
    "                loss = loss + tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n",
    "\n",
    "        network_vars = self.part_generator_trainable_variables\n",
    "        gradients = tape.gradient(loss, network_vars + [latent_code_vars])\n",
    "\n",
    "        self.optimizer_part_generator.apply_gradients(zip(gradients[:len(network_vars)], network_vars))\n",
    "        self.optimizer_latent_for_parts.apply_gradients(zip(gradients[len(network_vars):], [latent_code_vars]))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def distributed_train_step(self, train_func, args):\n",
    "        per_replica_losses = mirrored_strategy.run(train_func, args=args)\n",
    "        return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "    \n",
    "    def get_latent_code_vars_by_indices(self, latent_code_vars, latent_code_indices):\n",
    "        return tf.Variable(tf.gather(latent_code_vars, latent_code_indices), trainable=True)\n",
    "    \n",
    "    def update_latent_code_vars(self, indices, latent_code_vars):\n",
    "        self.latent_code_vars_parts.assign(tf.tensor_scatter_nd_update(self.latent_code_vars_parts, tf.expand_dims(indices, 1), latent_code_vars))\n",
    "\n",
    "    def save_models(self):\n",
    "        self.manager.save(checkpoint_number=self.trained_epoch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hparam = {\n",
    "    'model_latent_code_dim': 256,\n",
    "    'model_fc_channels': 512,\n",
    "    'model_conv_size': 4,\n",
    "    'model_num_latent_codes_parts': data_generator.num_parts,\n",
    "    'model_learning_rate_network': 5e-4,\n",
    "    'model_learning_rate_codes': 1e-3,\n",
    "    'model_voxel_map_shape': VOXEL_MAP_SHAPE,\n",
    "    'model_checkpoint_dir': './ckpt_dist',\n",
    "    'modelramdom_projection_num': 5\n",
    "}\n",
    "\n",
    "part_network = PartNetwork(model_hparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_EPOCH_FOR_PARTS = 1500\n",
    "\n",
    "for epoch in range(1, TRAINING_EPOCH_FOR_PARTS+1):\n",
    "    total_loss = []\n",
    "    \n",
    "    pbar = tqdm(dist_dataset, desc='Epoch [{}]'.format(epoch))\n",
    "    \n",
    "    for latent_code_indices, true_voxels_coords in pbar:\n",
    "        true_voxels_maps = mirrored_strategy.run(tf_data_to_dense, args=(true_voxels_coords))\n",
    "        \n",
    "        latent_code_vars = mirrored_strategy.run(part_network.get_latent_code_vars_by_indices, args=(part_network.latent_code_vars_parts, latent_code_indices))\n",
    "                            \n",
    "        loss = part_network.distributed_train_step(part_network.train_step_parts, (latent_code_vars, true_voxels_maps))\n",
    "        \n",
    "        mirrored_strategy.run(part_network.update_latent_code_vars, args=(latent_code_indices, latent_code_vars))\n",
    "                                \n",
    "        total_loss.append(loss)\n",
    "                \n",
    "        avg_loss = sum(total_loss) / len(total_loss)\n",
    "        \n",
    "        pbar.set_postfix({\"Avg Loss\": '{:.9f}'.format(avg_loss)})\n",
    "    \n",
    "part_network.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "no_improvement_count = 0\n",
    "max_no_improvement = 3\n",
    "\n",
    "while True:\n",
    "    total_loss = []\n",
    "    \n",
    "    pbar = tqdm(dist_dataset, desc='[Epoch {}]'.format(part_network.trained_epoch.numpy()), total=dataset_size)\n",
    "    \n",
    "    for latent_code_indices, true_voxels_coords in pbar:\n",
    "        true_voxels_maps = mirrored_strategy.run(tf_data_to_dense, args=(true_voxels_coords,))\n",
    "        latent_code_vars = mirrored_strategy.run(part_network.get_latent_code_vars_by_indices, args=(part_network.latent_code_vars_parts, latent_code_indices,))\n",
    "        \n",
    "        loss = part_network.distributed_train_step(part_network.train_step_parts, (latent_code_vars, true_voxels_maps))\n",
    "        \n",
    "        mirrored_strategy.run(part_network.update_latent_code_vars, args=(latent_code_indices, latent_code_vars,))\n",
    "        \n",
    "        total_loss.append(loss)\n",
    "    \n",
    "        avg_loss = sum(total_loss) / len(total_loss)\n",
    "        \n",
    "        pbar.set_postfix({'Avg Loss': '{:.9f}'.format(sum(total_loss) / len(total_loss))})\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    if no_improvement_count >= max_no_improvement:\n",
    "        print('Early stopping as loss has not improved for {} consecutive epochs.'.format(max_no_improvement))\n",
    "        break\n",
    "    \n",
    "    part_network.save_models()\n",
    "    part_network.trained_epoch.assign(part_network.trained_epoch+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
